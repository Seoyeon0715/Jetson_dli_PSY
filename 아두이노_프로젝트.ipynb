{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v4Wkqz3ThGwu"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import serial\n",
        "import time\n",
        "import json\n",
        "import gradio as gr\n",
        "from openai import OpenAI\n",
        "\n",
        "# 센서 데이터 처리 함수 정의\n",
        "\n",
        "# 토양 수분 센서\n",
        "def moisture_sensor_info(mode='real-time', file_path=None, port='/dev/ttyUSB0', baudrate=9600):\n",
        "    if mode == 'real-time':\n",
        "        try:\n",
        "            arduino = serial.Serial(port, baudrate, timeout=1)\n",
        "            if arduino.in_waiting > 0:\n",
        "                data = arduino.readline().decode('utf-8').strip()\n",
        "                timestamp = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "                arduino.close()\n",
        "                return str({'timestamp': timestamp, 'moisture': f\"{data}%\"})\n",
        "            else:\n",
        "                return str({'error': 'No data available from sensor.'})\n",
        "        except Exception as e:\n",
        "            return str({'error': str(e)})\n",
        "\n",
        "    elif mode == 'file':\n",
        "        try:\n",
        "            df = pd.read_excel(file_path)\n",
        "            if df.empty:\n",
        "                return str({'error': 'No data available in the file.'})\n",
        "            latest_entry = df.iloc[-1].to_dict()\n",
        "            return str({\n",
        "                'timestamp': latest_entry['Timestamp'],\n",
        "                'moisture': f\"{latest_entry['Moisture (%)']}%\"\n",
        "            })\n",
        "        except Exception as e:\n",
        "            return str({'error': str(e)})\n",
        "\n",
        "    else:\n",
        "        return str({'error': 'Invalid mode. Use \"real-time\" or \"file\".'})\n",
        "\n",
        "# 조도 센서\n",
        "def light_sensor_info(mode='real-time', file_path=None, port='/dev/ttyUSB0', baudrate=9600):\n",
        "    if mode == 'real-time':\n",
        "        try:\n",
        "            arduino = serial.Serial(port, baudrate, timeout=1)\n",
        "            time.sleep(2)  # 안정화 대기\n",
        "            if arduino.in_waiting > 0:\n",
        "                data = arduino.readline().decode(\"utf-8\").strip()\n",
        "                timestamp = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "                arduino.close()\n",
        "                return str({'timestamp': timestamp, 'brightness': f\"{data.split(': ')[1]}%\"})\n",
        "            else:\n",
        "                return str({'error': 'No data available from sensor.'})\n",
        "        except Exception as e:\n",
        "            return str({'error': str(e)})\n",
        "\n",
        "    elif mode == 'file':\n",
        "        try:\n",
        "            df = pd.read_excel(file_path)\n",
        "            if df.empty:\n",
        "                return str({'error': 'No data available in the file.'})\n",
        "            latest_entry = df.iloc[-1].to_dict()\n",
        "            return str({\n",
        "                'timestamp': latest_entry['Timestamp'],\n",
        "                'brightness': f\"{latest_entry['Brightness (%)']}%\"\n",
        "            })\n",
        "        except Exception as e:\n",
        "            return str({'error': str(e)})\n",
        "\n",
        "    else:\n",
        "        return str({'error': 'Invalid mode. Use \"real-time\" or \"file\".'})\n",
        "\n",
        "# 온습도 센서\n",
        "def dht11_sensor_info(mode='real-time', file_path=None, port='/dev/ttyUSB0', baudrate=9600):\n",
        "    if mode == 'real-time':\n",
        "        try:\n",
        "            arduino = serial.Serial(port, baudrate, timeout=1)\n",
        "            if arduino.in_waiting > 0:\n",
        "                data = arduino.readline().decode('utf-8').strip()\n",
        "                temperature, humidity = data.split(',')\n",
        "                timestamp = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "                arduino.close()\n",
        "                return str({\n",
        "                    'timestamp': timestamp,\n",
        "                    'temperature': f\"{temperature.strip()}°C\",\n",
        "                    'humidity': f\"{humidity.strip()}%\"\n",
        "                })\n",
        "            else:\n",
        "                return str({'error': 'No data available from sensor.'})\n",
        "        except Exception as e:\n",
        "            return str({'error': str(e)})\n",
        "\n",
        "    elif mode == 'file':\n",
        "        try:\n",
        "            df = pd.read_excel(file_path)\n",
        "            if df.empty:\n",
        "                return str({'error': 'No data available in the file.'})\n",
        "            latest_entry = df.iloc[-1].to_dict()\n",
        "            return str({\n",
        "                'timestamp': latest_entry['Timestamp'],\n",
        "                'temperature': f\"{latest_entry['Temperature (°C)']}°C\",\n",
        "                'humidity': f\"{latest_entry['Humidity (%)']}%\"\n",
        "            })\n",
        "        except Exception as e:\n",
        "            return str({'error': str(e)})\n",
        "\n",
        "    else:\n",
        "        return str({'error': 'Invalid mode. Use \"real-time\" or \"file\".'})\n",
        "\n",
        "# 함수 묘사 (use_functions)\n",
        "use_functions = [\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"moisture_sensor_info\",\n",
        "            \"description\": \"Retrieve soil moisture data either in real-time or from a file.\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"mode\": {\"type\": \"string\", \"enum\": [\"real-time\", \"file\"]},\n",
        "                    \"file_path\": {\"type\": \"string\", \"nullable\": True},\n",
        "                    \"port\": {\"type\": \"string\", \"nullable\": True},\n",
        "                    \"baudrate\": {\"type\": \"integer\", \"nullable\": True}\n",
        "                },\n",
        "                \"required\": [\"mode\"]\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"light_sensor_info\",\n",
        "            \"description\": \"Retrieve light sensor data either in real-time or from a file.\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"mode\": {\"type\": \"string\", \"enum\": [\"real-time\", \"file\"]},\n",
        "                    \"file_path\": {\"type\": \"string\", \"nullable\": True},\n",
        "                    \"port\": {\"type\": \"string\", \"nullable\": True},\n",
        "                    \"baudrate\": {\"type\": \"integer\", \"nullable\": True}\n",
        "                },\n",
        "                \"required\": [\"mode\"]\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"dht11_sensor_info\",\n",
        "            \"description\": \"Retrieve temperature and humidity data from a DHT11 sensor.\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"mode\": {\"type\": \"string\", \"enum\": [\"real-time\", \"file\"]},\n",
        "                    \"file_path\": {\"type\": \"string\", \"nullable\": True},\n",
        "                    \"port\": {\"type\": \"string\", \"nullable\": True},\n",
        "                    \"baudrate\": {\"type\": \"integer\", \"nullable\": True}\n",
        "                },\n",
        "                \"required\": [\"mode\"]\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "]\n",
        "\n",
        "# OpenAI 연동 함수\n",
        "def ask_openai(llm_model, messages, user_message, functions=''):\n",
        "    client = OpenAI()\n",
        "    proc_messages = messages\n",
        "\n",
        "    if user_message:\n",
        "        proc_messages.append({\"role\": \"user\", \"content\": user_message})\n",
        "\n",
        "    if not functions:\n",
        "        response = client.chat.completions.create(model=llm_model, messages=proc_messages, temperature=1.0)\n",
        "    else:\n",
        "        response = client.chat.completions.create(model=llm_model, messages=proc_messages, tools=functions, tool_choice=\"auto\")\n",
        "\n",
        "    response_message = response.choices[0].message\n",
        "    tool_calls = response_message.tool_calls\n",
        "\n",
        "    if tool_calls:\n",
        "        available_functions = {\n",
        "            \"moisture_sensor_info\": moisture_sensor_info,\n",
        "            \"light_sensor_info\": light_sensor_info,\n",
        "            \"dht11_sensor_info\": dht11_sensor_info\n",
        "        }\n",
        "        for tool_call in tool_calls:\n",
        "            function_name = tool_call.function.name\n",
        "            function_to_call = available_functions[function_name]\n",
        "            function_args = json.loads(tool_call.function.arguments)\n",
        "            function_response = function_to_call(**function_args) if function_args else function_to_call()\n",
        "            proc_messages.append(\n",
        "                {\"tool_call_id\": tool_call.id, \"role\": \"tool\", \"name\": function_name, \"content\": function_response}\n",
        "            )\n",
        "        second_response = client.chat.completions.create(\n",
        "            model=llm_model,\n",
        "            messages=proc_messages,\n",
        "        )\n",
        "        assistant_message = second_response.choices[0].message.content\n",
        "    else:\n",
        "        assistant_message = response_message.content\n",
        "\n",
        "    proc_messages.append({\"role\": \"assistant\", \"content\": assistant_message})\n",
        "    return proc_messages, assistant_message.strip()\n",
        "\n",
        "# Gradio 인터페이스\n",
        "messages = []\n",
        "\n",
        "def process(user_message, chat_history):\n",
        "    proc_messages, ai_message = ask_openai(\"gpt-4o-mini\", messages, user_message, functions=use_functions)\n",
        "    chat_history.append((user_message, ai_message))\n",
        "    return \"\", chat_history\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    chatbot = gr.Chatbot(label=\"센서 데이터 챗봇\")\n",
        "    user_textbox = gr.Textbox(label=\"입력\")\n",
        "    user_textbox.submit(process, [user_textbox, chatbot], [user_textbox, chatbot])\n",
        "\n",
        "demo.launch(share=True, debug=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "55jXrld0jTvU",
        "outputId": "4a855d92-586d-435d-dcb9-a2a738257721"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gradio/components/chatbot.py:237: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://5d60b728d64d093a03.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://5d60b728d64d093a03.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/queueing.py\", line 624, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/route_utils.py\", line 323, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 2043, in process_api\n",
            "    result = await self.call_function(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1590, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/to_thread.py\", line 33, in run_sync\n",
            "    return await get_asynclib().run_sync_in_worker_thread(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 807, in run\n",
            "    result = context.run(func, *args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/utils.py\", line 865, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "  File \"<ipython-input-5-92e2da57d5c5>\", line 191, in process\n",
            "    proc_messages, ai_message = ask_openai(\"gpt-4o-mini\", messages, user_message, functions=use_functions)\n",
            "  File \"<ipython-input-5-92e2da57d5c5>\", line 148, in ask_openai\n",
            "    client = OpenAI()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_client.py\", line 105, in __init__\n",
            "    raise OpenAIError(\n",
            "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://5d60b728d64d093a03.gradio.live\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import serial\n",
        "import time\n",
        "import json\n",
        "import gradio as gr\n",
        "from openai import OpenAI\n",
        "\n",
        "# 센서 데이터 처리 함수 정의\n",
        "\n",
        "# 토양 수분 센서\n",
        "def moisture_sensor_info(mode='real-time', file_path=None, port='/dev/ttyUSB0', baudrate=9600):\n",
        "    if mode == 'real-time':\n",
        "        try:\n",
        "            # 센서 데이터를 시뮬레이션\n",
        "            data = \"45\"  # 임의의 수분 데이터 (예: 45%)\n",
        "            timestamp = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "            return str({'timestamp': timestamp, 'moisture': f\"{data}%\"})\n",
        "        except Exception as e:\n",
        "            return str({'error': str(e)})\n",
        "    elif mode == 'file':\n",
        "        try:\n",
        "            df = pd.read_excel(file_path)\n",
        "            if df.empty:\n",
        "                return str({'error': 'No data available in the file.'})\n",
        "            latest_entry = df.iloc[-1].to_dict()\n",
        "            return str({\n",
        "                'timestamp': latest_entry['Timestamp'],\n",
        "                'moisture': f\"{latest_entry['Moisture (%)']}%\"\n",
        "            })\n",
        "        except Exception as e:\n",
        "            return str({'error': str(e)})\n",
        "    else:\n",
        "        return str({'error': 'Invalid mode. Use \"real-time\" or \"file\".'})\n",
        "\n",
        "# 조도 센서\n",
        "def light_sensor_info(mode='real-time', file_path=None, port='/dev/ttyUSB0', baudrate=9600):\n",
        "    if mode == 'real-time':\n",
        "        try:\n",
        "            # 센서 데이터를 시뮬레이션\n",
        "            data = \"85\"  # 임의의 밝기 데이터 (예: 85%)\n",
        "            timestamp = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "            return str({'timestamp': timestamp, 'brightness': f\"{data}%\"})\n",
        "        except Exception as e:\n",
        "            return str({'error': str(e)})\n",
        "    elif mode == 'file':\n",
        "        try:\n",
        "            df = pd.read_excel(file_path)\n",
        "            if df.empty:\n",
        "                return str({'error': 'No data available in the file.'})\n",
        "            latest_entry = df.iloc[-1].to_dict()\n",
        "            return str({\n",
        "                'timestamp': latest_entry['Timestamp'],\n",
        "                'brightness': f\"{latest_entry['Brightness (%)']}%\"\n",
        "            })\n",
        "        except Exception as e:\n",
        "            return str({'error': str(e)})\n",
        "    else:\n",
        "        return str({'error': 'Invalid mode. Use \"real-time\" or \"file\".'})\n",
        "\n",
        "# 온습도 센서\n",
        "def dht11_sensor_info(mode='real-time', file_path=None, port='/dev/ttyUSB0', baudrate=9600):\n",
        "    if mode == 'real-time':\n",
        "        try:\n",
        "            # 센서 데이터를 시뮬레이션\n",
        "            temperature = \"22\"  # 임의의 온도 데이터 (예: 22°C)\n",
        "            humidity = \"55\"  # 임의의 습도 데이터 (예: 55%)\n",
        "            timestamp = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "            return str({\n",
        "                'timestamp': timestamp,\n",
        "                'temperature': f\"{temperature.strip()}°C\",\n",
        "                'humidity': f\"{humidity.strip()}%\"\n",
        "            })\n",
        "        except Exception as e:\n",
        "            return str({'error': str(e)})\n",
        "    elif mode == 'file':\n",
        "        try:\n",
        "            df = pd.read_excel(file_path)\n",
        "            if df.empty:\n",
        "                return str({'error': 'No data available in the file.'})\n",
        "            latest_entry = df.iloc[-1].to_dict()\n",
        "            return str({\n",
        "                'timestamp': latest_entry['Timestamp'],\n",
        "                'temperature': f\"{latest_entry['Temperature (°C)']}°C\",\n",
        "                'humidity': f\"{latest_entry['Humidity (%)']}%\"\n",
        "            })\n",
        "        except Exception as e:\n",
        "            return str({'error': str(e)})\n",
        "    else:\n",
        "        return str({'error': 'Invalid mode. Use \"real-time\" or \"file\".'})\n",
        "\n",
        "# 함수 묘사 (use_functions)\n",
        "use_functions = [\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"moisture_sensor_info\",\n",
        "            \"description\": \"Retrieve soil moisture data either in real-time or from a file.\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"mode\": {\"type\": \"string\", \"enum\": [\"real-time\", \"file\"]},\n",
        "                    \"file_path\": {\"type\": \"string\", \"nullable\": True},\n",
        "                    \"port\": {\"type\": \"string\", \"nullable\": True},\n",
        "                    \"baudrate\": {\"type\": \"integer\", \"nullable\": True}\n",
        "                },\n",
        "                \"required\": [\"mode\"]\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"light_sensor_info\",\n",
        "            \"description\": \"Retrieve light sensor data either in real-time or from a file.\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"mode\": {\"type\": \"string\", \"enum\": [\"real-time\", \"file\"]},\n",
        "                    \"file_path\": {\"type\": \"string\", \"nullable\": True},\n",
        "                    \"port\": {\"type\": \"string\", \"nullable\": True},\n",
        "                    \"baudrate\": {\"type\": \"integer\", \"nullable\": True}\n",
        "                },\n",
        "                \"required\": [\"mode\"]\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"dht11_sensor_info\",\n",
        "            \"description\": \"Retrieve temperature and humidity data from a DHT11 sensor.\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"mode\": {\"type\": \"string\", \"enum\": [\"real-time\", \"file\"]},\n",
        "                    \"file_path\": {\"type\": \"string\", \"nullable\": True},\n",
        "                    \"port\": {\"type\": \"string\", \"nullable\": True},\n",
        "                    \"baudrate\": {\"type\": \"integer\", \"nullable\": True}\n",
        "                },\n",
        "                \"required\": [\"mode\"]\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "]\n",
        "\n",
        "# OpenAI 연동 함수\n",
        "def ask_openai(llm_model, messages, user_message, functions=''):\n",
        "    client = OpenAI()\n",
        "    proc_messages = messages\n",
        "\n",
        "    if user_message:\n",
        "        proc_messages.append({\"role\": \"user\", \"content\": user_message})\n",
        "\n",
        "    if not functions:\n",
        "        response = client.chat.completions.create(model=llm_model, messages=proc_messages, temperature=1.0)\n",
        "    else:\n",
        "        response = client.chat.completions.create(model=llm_model, messages=proc_messages, tools=functions, tool_choice=\"auto\")\n",
        "\n",
        "    response_message = response.choices[0].message\n",
        "    tool_calls = response_message.tool_calls\n",
        "\n",
        "    if tool_calls:\n",
        "        available_functions = {\n",
        "            \"moisture_sensor_info\": moisture_sensor_info,\n",
        "            \"light_sensor_info\": light_sensor_info,\n",
        "            \"dht11_sensor_info\": dht11_sensor_info\n",
        "        }\n",
        "        for tool_call in tool_calls:\n",
        "            function_name = tool_call.function.name\n",
        "            function_to_call = available_functions[function_name]\n",
        "            function_args = json.loads(tool_call.function.arguments)\n",
        "            function_response = function_to_call(**function_args) if function_args else function_to_call()\n",
        "            proc_messages.append(\n",
        "                {\"tool_call_id\": tool_call.id, \"role\": \"tool\", \"name\": function_name, \"content\": function_response}\n",
        "            )\n",
        "        second_response = client.chat.completions.create(\n",
        "            model=llm_model,\n",
        "            messages=proc_messages,\n",
        "        )\n",
        "        assistant_message = second_response.choices[0].message.content\n",
        "    else:\n",
        "        assistant_message = response_message.content\n",
        "\n",
        "    proc_messages.append({\"role\": \"assistant\", \"content\": assistant_message})\n",
        "    return proc_messages, assistant_message.strip()\n",
        "\n",
        "# Gradio 인터페이스\n",
        "messages = []\n",
        "\n",
        "def process(user_message, chat_history):\n",
        "    proc_messages, ai_message = ask_openai(\"gpt-4o-mini\", messages, user_message, functions=use_functions)\n",
        "    chat_history.append((user_message, ai_message))\n",
        "    return \"\", chat_history\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    chatbot = gr.Chatbot(label=\"센서 데이터 챗봇\")\n",
        "    user_textbox = gr.Textbox(label=\"입력\")\n",
        "    user_textbox.submit(process, [user_textbox, chatbot], [user_textbox, chatbot])\n",
        "\n",
        "demo.launch(share=True, debug=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        },
        "id": "_KXlm-sRk99l",
        "outputId": "cc056c5f-02f9-458e-f987-113e194dce1e"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'serial'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-90446f0ca1d0>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mserial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgradio\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'serial'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import serial\n",
        "import time\n",
        "import json\n",
        "import gradio as gr\n",
        "import openai\n",
        "\n",
        "# OpenAI API 키 설정\n",
        "openai.api_key = \"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\"  # 여기에서 \"your_openai_api_key\"를 실제 API 키로 바꾸세요.\n",
        "\n",
        "# 센서 데이터 처리 함수 정의\n",
        "\n",
        "# 토양 수분 센서\n",
        "def moisture_sensor_info(mode='real-time', file_path=None, port='/dev/ttyUSB0', baudrate=9600):\n",
        "    if mode == 'real-time':\n",
        "        try:\n",
        "            # 센서 데이터를 시뮬레이션\n",
        "            data = \"45\"  # 임의의 수분 데이터 (예: 45%)\n",
        "            timestamp = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "            return str({'timestamp': timestamp, 'moisture': f\"{data}%\"})\n",
        "        except Exception as e:\n",
        "            return str({'error': str(e)})\n",
        "    elif mode == 'file':\n",
        "        try:\n",
        "            df = pd.read_excel(file_path)\n",
        "            if df.empty:\n",
        "                return str({'error': 'No data available in the file.'})\n",
        "            latest_entry = df.iloc[-1].to_dict()\n",
        "            return str({\n",
        "                'timestamp': latest_entry['Timestamp'],\n",
        "                'moisture': f\"{latest_entry['Moisture (%)']}%\"\n",
        "            })\n",
        "        except Exception as e:\n",
        "            return str({'error': str(e)})\n",
        "    else:\n",
        "        return str({'error': 'Invalid mode. Use \"real-time\" or \"file\".'})\n",
        "\n",
        "# 조도 센서\n",
        "def light_sensor_info(mode='real-time', file_path=None, port='/dev/ttyUSB0', baudrate=9600):\n",
        "    if mode == 'real-time':\n",
        "        try:\n",
        "            # 센서 데이터를 시뮬레이션\n",
        "            data = \"85\"  # 임의의 밝기 데이터 (예: 85%)\n",
        "            timestamp = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "            return str({'timestamp': timestamp, 'brightness': f\"{data}%\"})\n",
        "        except Exception as e:\n",
        "            return str({'error': str(e)})\n",
        "    elif mode == 'file':\n",
        "        try:\n",
        "            df = pd.read_excel(file_path)\n",
        "            if df.empty:\n",
        "                return str({'error': 'No data available in the file.'})\n",
        "            latest_entry = df.iloc[-1].to_dict()\n",
        "            return str({\n",
        "                'timestamp': latest_entry['Timestamp'],\n",
        "                'brightness': f\"{latest_entry['Brightness (%)']}%\"\n",
        "            })\n",
        "        except Exception as e:\n",
        "            return str({'error': str(e)})\n",
        "    else:\n",
        "        return str({'error': 'Invalid mode. Use \"real-time\" or \"file\".'})\n",
        "\n",
        "# 온습도 센서\n",
        "def dht11_sensor_info(mode='real-time', file_path=None, port='/dev/ttyUSB0', baudrate=9600):\n",
        "    if mode == 'real-time':\n",
        "        try:\n",
        "            # 센서 데이터를 시뮬레이션\n",
        "            temperature = \"22\"  # 임의의 온도 데이터 (예: 22°C)\n",
        "            humidity = \"55\"  # 임의의 습도 데이터 (예: 55%)\n",
        "            timestamp = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "            return str({\n",
        "                'timestamp': timestamp,\n",
        "                'temperature': f\"{temperature.strip()}°C\",\n",
        "                'humidity': f\"{humidity.strip()}%\"\n",
        "            })\n",
        "        except Exception as e:\n",
        "            return str({'error': str(e)})\n",
        "    elif mode == 'file':\n",
        "        try:\n",
        "            df = pd.read_excel(file_path)\n",
        "            if df.empty:\n",
        "                return str({'error': 'No data available in the file.'})\n",
        "            latest_entry = df.iloc[-1].to_dict()\n",
        "            return str({\n",
        "                'timestamp': latest_entry['Timestamp'],\n",
        "                'temperature': f\"{latest_entry['Temperature (°C)']}°C\",\n",
        "                'humidity': f\"{latest_entry['Humidity (%)']}%\"\n",
        "            })\n",
        "        except Exception as e:\n",
        "            return str({'error': str(e)})\n",
        "    else:\n",
        "        return str({'error': 'Invalid mode. Use \"real-time\" or \"file\".'})\n",
        "\n",
        "# 함수 묘사 (use_functions)\n",
        "use_functions = [\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"moisture_sensor_info\",\n",
        "            \"description\": \"Retrieve soil moisture data either in real-time or from a file.\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"mode\": {\"type\": \"string\", \"enum\": [\"real-time\", \"file\"]},\n",
        "                    \"file_path\": {\"type\": \"string\", \"nullable\": True},\n",
        "                    \"port\": {\"type\": \"string\", \"nullable\": True},\n",
        "                    \"baudrate\": {\"type\": \"integer\", \"nullable\": True}\n",
        "                },\n",
        "                \"required\": [\"mode\"]\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"light_sensor_info\",\n",
        "            \"description\": \"Retrieve light sensor data either in real-time or from a file.\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"mode\": {\"type\": \"string\", \"enum\": [\"real-time\", \"file\"]},\n",
        "                    \"file_path\": {\"type\": \"string\", \"nullable\": True},\n",
        "                    \"port\": {\"type\": \"string\", \"nullable\": True},\n",
        "                    \"baudrate\": {\"type\": \"integer\", \"nullable\": True}\n",
        "                },\n",
        "                \"required\": [\"mode\"]\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"dht11_sensor_info\",\n",
        "            \"description\": \"Retrieve temperature and humidity data from a DHT11 sensor.\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"mode\": {\"type\": \"string\", \"enum\": [\"real-time\", \"file\"]},\n",
        "                    \"file_path\": {\"type\": \"string\", \"nullable\": True},\n",
        "                    \"port\": {\"type\": \"string\", \"nullable\": True},\n",
        "                    \"baudrate\": {\"type\": \"integer\", \"nullable\": True}\n",
        "                },\n",
        "                \"required\": [\"mode\"]\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "]\n",
        "\n",
        "# OpenAI 연동 함수\n",
        "def ask_openai(llm_model, messages, user_message, functions=''):\n",
        "    proc_messages = messages\n",
        "\n",
        "    if user_message:\n",
        "        proc_messages.append({\"role\": \"user\", \"content\": user_message})\n",
        "\n",
        "    if not functions:\n",
        "        response = openai.ChatCompletion.create(model=llm_model, messages=proc_messages, temperature=1.0)\n",
        "    else:\n",
        "        response = openai.ChatCompletion.create(model=llm_model, messages=proc_messages, functions=functions, function_call=\"auto\")\n",
        "\n",
        "    response_message = response.choices[0].message\n",
        "    tool_calls = getattr(response_message, \"function_call\", None)\n",
        "\n",
        "    if tool_calls:\n",
        "        available_functions = {\n",
        "            \"moisture_sensor_info\": moisture_sensor_info,\n",
        "            \"light_sensor_info\": light_sensor_info,\n",
        "            \"dht11_sensor_info\": dht11_sensor_info\n",
        "        }\n",
        "        for tool_call in tool_calls:\n",
        "            function_name = tool_call[\"name\"]\n",
        "            function_to_call = available_functions[function_name]\n",
        "            function_args = json.loads(tool_call[\"arguments\"])\n",
        "            function_response = function_to_call(**function_args) if function_args else function_to_call()\n",
        "            proc_messages.append(\n",
        "                {\"role\": \"function\", \"name\": function_name, \"content\": function_response}\n",
        "            )\n",
        "        second_response = openai.ChatCompletion.create(model=llm_model, messages=proc_messages)\n",
        "        assistant_message = second_response.choices[0].message.content\n",
        "    else:\n",
        "        assistant_message = response_message.content\n",
        "\n",
        "    proc_messages.append({\"role\": \"assistant\", \"content\": assistant_message})\n",
        "    return proc_messages, assistant_message.strip()\n",
        "\n",
        "# Gradio 인터페이스\n",
        "messages = []\n",
        "\n",
        "def process(user_message, chat_history):\n",
        "    proc_messages, ai_message = ask_openai(\"gpt-4o-mini\", messages, user_message, functions=use_functions)\n",
        "    chat_history.append((user_message, ai_message))\n",
        "    return \"\", chat_history\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    chatbot = gr.Chatbot(label=\"센서 데이터 챗봇\")\n",
        "    user_textbox = gr.Textbox(label=\"입력\")\n",
        "    user_textbox.submit(process, [user_textbox, chatbot], [user_textbox, chatbot])\n",
        "\n",
        "demo.launch(share=True, debug=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "GB8DEUWQmFAk",
        "outputId": "5b713f9d-7da9-44a2-e0c4-07a19f33caea"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gradio/components/chatbot.py:237: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://73d54e8c5fba19e40d.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://73d54e8c5fba19e40d.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/queueing.py\", line 624, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/route_utils.py\", line 323, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 2043, in process_api\n",
            "    result = await self.call_function(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1590, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/to_thread.py\", line 33, in run_sync\n",
            "    return await get_asynclib().run_sync_in_worker_thread(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 807, in run\n",
            "    result = context.run(func, *args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/utils.py\", line 865, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "  File \"<ipython-input-9-30235c021642>\", line 105, in process\n",
            "    proc_messages, ai_message = ask_openai(\"gpt-4-0613\", messages, user_message, functions=use_functions)\n",
            "  File \"<ipython-input-9-30235c021642>\", line 74, in ask_openai\n",
            "    response = openai.ChatCompletion.create(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/lib/_old_api.py\", line 39, in __call__\n",
            "    raise APIRemovedInV1(symbol=self._symbol)\n",
            "openai.lib._old_api.APIRemovedInV1: \n",
            "\n",
            "You tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n",
            "\n",
            "You can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n",
            "\n",
            "Alternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n",
            "\n",
            "A detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://73d54e8c5fba19e40d.gradio.live\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# openai 버전이 맞지 않아 최신버전으로 수정한 코드, 이건 open ai 인터페이스가 1.0.0 이상 버전에서 변경됨, 코드가 구버전이라 최신 방식코드를\n",
        "#작성하거나 아니면 open ai 라이브러리의 이전 버전으로 설치해야함\n",
        "import pandas as pd\n",
        "import time\n",
        "import json\n",
        "import gradio as gr\n",
        "import openai\n",
        "\n",
        "# OpenAI API 키 설정\n",
        "openai.api_key = \"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\"\n",
        "\n",
        "# 센서 데이터 처리 함수 정의\n",
        "def moisture_sensor_info(mode='real-time', file_path=None, port='/dev/ttyUSB0', baudrate=9600):\n",
        "    if mode == 'real-time':\n",
        "        data = \"45\"\n",
        "        timestamp = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "        return str({'timestamp': timestamp, 'moisture': f\"{data}%\"})\n",
        "    elif mode == 'file':\n",
        "        df = pd.read_excel(file_path)\n",
        "        latest_entry = df.iloc[-1].to_dict()\n",
        "        return str({'timestamp': latest_entry['Timestamp'], 'moisture': f\"{latest_entry['Moisture (%)']}%\"})\n",
        "    else:\n",
        "        return str({'error': 'Invalid mode'})\n",
        "\n",
        "def light_sensor_info(mode='real-time', file_path=None, port='/dev/ttyUSB0', baudrate=9600):\n",
        "    if mode == 'real-time':\n",
        "        data = \"85\"\n",
        "        timestamp = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "        return str({'timestamp': timestamp, 'brightness': f\"{data}%\"})\n",
        "    elif mode == 'file':\n",
        "        df = pd.read_excel(file_path)\n",
        "        latest_entry = df.iloc[-1].to_dict()\n",
        "        return str({'timestamp': latest_entry['Timestamp'], 'brightness': f\"{latest_entry['Brightness (%)']}%\"})\n",
        "    else:\n",
        "        return str({'error': 'Invalid mode'})\n",
        "\n",
        "def dht11_sensor_info(mode='real-time', file_path=None, port='/dev/ttyUSB0', baudrate=9600):\n",
        "    if mode == 'real-time':\n",
        "        temperature = \"22\"\n",
        "        humidity = \"55\"\n",
        "        timestamp = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "        return str({'timestamp': timestamp, 'temperature': f\"{temperature}°C\", 'humidity': f\"{humidity}%\"})\n",
        "    elif mode == 'file':\n",
        "        df = pd.read_excel(file_path)\n",
        "        latest_entry = df.iloc[-1].to_dict()\n",
        "        return str({'timestamp': latest_entry['Timestamp'], 'temperature': f\"{latest_entry['Temperature (°C)']}°C\", 'humidity': f\"{latest_entry['Humidity (%)']}%\"})\n",
        "    else:\n",
        "        return str({'error': 'Invalid mode'})\n",
        "\n",
        "use_functions = [\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"moisture_sensor_info\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\"mode\": {\"type\": \"string\", \"enum\": [\"real-time\", \"file\"]}}\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "]\n",
        "\n",
        "def ask_openai(llm_model, messages, user_message, functions=''):\n",
        "    proc_messages = messages\n",
        "\n",
        "    if user_message:\n",
        "        proc_messages.append({\"role\": \"user\", \"content\": user_message})\n",
        "\n",
        "    if not functions:\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=llm_model, messages=proc_messages, temperature=1.0\n",
        "        )\n",
        "    else:\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=llm_model, messages=proc_messages, functions=functions, function_call=\"auto\"\n",
        "        )\n",
        "\n",
        "    response_message = response.choices[0].message\n",
        "    tool_calls = response_message.get(\"function_call\")\n",
        "\n",
        "    if tool_calls:\n",
        "        available_functions = {\n",
        "            \"moisture_sensor_info\": moisture_sensor_info,\n",
        "            \"light_sensor_info\": light_sensor_info,\n",
        "            \"dht11_sensor_info\": dht11_sensor_info,\n",
        "        }\n",
        "        function_name = tool_calls[\"name\"]\n",
        "        function_to_call = available_functions[function_name]\n",
        "        function_args = json.loads(tool_calls[\"arguments\"])\n",
        "        function_response = function_to_call(**function_args) if function_args else function_to_call()\n",
        "\n",
        "        proc_messages.append({\"role\": \"function\", \"name\": function_name, \"content\": function_response})\n",
        "        second_response = openai.ChatCompletion.create(model=llm_model, messages=proc_messages)\n",
        "        assistant_message = second_response.choices[0].message[\"content\"]\n",
        "    else:\n",
        "        assistant_message = response_message[\"content\"]\n",
        "\n",
        "    proc_messages.append({\"role\": \"assistant\", \"content\": assistant_message})\n",
        "    return proc_messages, assistant_message.strip()\n",
        "\n",
        "# Gradio 인터페이스\n",
        "messages = []\n",
        "\n",
        "def process(user_message, chat_history):\n",
        "    proc_messages, ai_message = ask_openai(\"gpt-4-0613\", messages, user_message, functions=use_functions)\n",
        "    chat_history.append((user_message, ai_message))\n",
        "    return \"\", chat_history\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    chatbot = gr.Chatbot(label=\"센서 데이터 챗봇\")\n",
        "    user_textbox = gr.Textbox(label=\"입력\")\n",
        "    user_textbox.submit(process, [user_textbox, chatbot], [user_textbox, chatbot])\n",
        "\n",
        "demo.launch(share=True, debug=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "MfHpe_DumKlK",
        "outputId": "a7431adf-d520-49ce-aa4d-d826ac40d49c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gradio/components/chatbot.py:237: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://a3f03e5a5cbf79a617.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://a3f03e5a5cbf79a617.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/queueing.py\", line 624, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/route_utils.py\", line 323, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 2043, in process_api\n",
            "    result = await self.call_function(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1590, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/to_thread.py\", line 33, in run_sync\n",
            "    return await get_asynclib().run_sync_in_worker_thread(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 807, in run\n",
            "    result = context.run(func, *args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/utils.py\", line 865, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "  File \"<ipython-input-11-5df8fae82924>\", line 113, in process\n",
            "    proc_messages, ai_message = ask_openai(\"gpt-4\", messages, user_message, functions=use_functions)\n",
            "  File \"<ipython-input-11-5df8fae82924>\", line 80, in ask_openai\n",
            "    response = openai.ChatCompletion.create(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/lib/_old_api.py\", line 39, in __call__\n",
            "    raise APIRemovedInV1(symbol=self._symbol)\n",
            "openai.lib._old_api.APIRemovedInV1: \n",
            "\n",
            "You tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n",
            "\n",
            "You can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n",
            "\n",
            "Alternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n",
            "\n",
            "A detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://a3f03e5a5cbf79a617.gradio.live\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#일단 위 코드를 최신 open ai python sdk에 맞게 업데이트한것, 실패함\n",
        "\n",
        "import pandas as pd\n",
        "import time\n",
        "import json\n",
        "import gradio as gr\n",
        "import openai\n",
        "\n",
        "# OpenAI API 키 설정\n",
        "openai.api_key = \"your_openai_api_key\"\n",
        "\n",
        "# 센서 데이터 처리 함수 정의\n",
        "def moisture_sensor_info(mode='real-time', file_path=None, port='/dev/ttyUSB0', baudrate=9600):\n",
        "    if mode == 'real-time':\n",
        "        data = \"45\"\n",
        "        timestamp = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "        return str({'timestamp': timestamp, 'moisture': f\"{data}%\"})\n",
        "    elif mode == 'file':\n",
        "        df = pd.read_excel(file_path)\n",
        "        latest_entry = df.iloc[-1].to_dict()\n",
        "        return str({'timestamp': latest_entry['Timestamp'], 'moisture': f\"{latest_entry['Moisture (%)']}%\"})\n",
        "    else:\n",
        "        return str({'error': 'Invalid mode'})\n",
        "\n",
        "def light_sensor_info(mode='real-time', file_path=None, port='/dev/ttyUSB0', baudrate=9600):\n",
        "    if mode == 'real-time':\n",
        "        data = \"85\"\n",
        "        timestamp = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "        return str({'timestamp': timestamp, 'brightness': f\"{data}%\"})\n",
        "    elif mode == 'file':\n",
        "        df = pd.read_excel(file_path)\n",
        "        latest_entry = df.iloc[-1].to_dict()\n",
        "        return str({'timestamp': latest_entry['Timestamp'], 'brightness': f\"{latest_entry['Brightness (%)']}%\"})\n",
        "    else:\n",
        "        return str({'error': 'Invalid mode'})\n",
        "\n",
        "def dht11_sensor_info(mode='real-time', file_path=None, port='/dev/ttyUSB0', baudrate=9600):\n",
        "    if mode == 'real-time':\n",
        "        temperature = \"22\"\n",
        "        humidity = \"55\"\n",
        "        timestamp = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "        return str({'timestamp': timestamp, 'temperature': f\"{temperature}°C\", 'humidity': f\"{humidity}%\"})\n",
        "    elif mode == 'file':\n",
        "        df = pd.read_excel(file_path)\n",
        "        latest_entry = df.iloc[-1].to_dict()\n",
        "        return str({'timestamp': latest_entry['Timestamp'], 'temperature': f\"{latest_entry['Temperature (°C)']}°C\", 'humidity': f\"{latest_entry['Humidity (%)']}%\"})\n",
        "    else:\n",
        "        return str({'error': 'Invalid mode'})\n",
        "\n",
        "use_functions = [\n",
        "    {\n",
        "        \"name\": \"moisture_sensor_info\",\n",
        "        \"description\": \"Retrieve soil moisture data either in real-time or from a file.\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"mode\": {\"type\": \"string\", \"enum\": [\"real-time\", \"file\"]},\n",
        "                \"file_path\": {\"type\": \"string\", \"nullable\": True},\n",
        "                \"port\": {\"type\": \"string\", \"nullable\": True},\n",
        "                \"baudrate\": {\"type\": \"integer\", \"nullable\": True}\n",
        "            },\n",
        "            \"required\": [\"mode\"]\n",
        "        }\n",
        "    }\n",
        "]\n",
        "\n",
        "def ask_openai(llm_model, messages, user_message, functions=None):\n",
        "    proc_messages = messages\n",
        "\n",
        "    if user_message:\n",
        "        proc_messages.append({\"role\": \"user\", \"content\": user_message})\n",
        "\n",
        "    if not functions:\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=llm_model,\n",
        "            messages=proc_messages,\n",
        "            temperature=1.0\n",
        "        )\n",
        "    else:\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=llm_model,\n",
        "            messages=proc_messages,\n",
        "            functions=functions,\n",
        "            function_call=\"auto\"\n",
        "        )\n",
        "\n",
        "    response_message = response[\"choices\"][0][\"message\"]\n",
        "    function_call = response_message.get(\"function_call\")\n",
        "\n",
        "    if function_call:\n",
        "        available_functions = {\n",
        "            \"moisture_sensor_info\": moisture_sensor_info,\n",
        "            \"light_sensor_info\": light_sensor_info,\n",
        "            \"dht11_sensor_info\": dht11_sensor_info\n",
        "        }\n",
        "        function_name = function_call[\"name\"]\n",
        "        function_to_call = available_functions[function_name]\n",
        "        function_args = json.loads(function_call[\"arguments\"])\n",
        "        function_response = function_to_call(**function_args)\n",
        "        proc_messages.append({\"role\": \"function\", \"name\": function_name, \"content\": function_response})\n",
        "        second_response = openai.ChatCompletion.create(model=llm_model, messages=proc_messages)\n",
        "        assistant_message = second_response[\"choices\"][0][\"message\"][\"content\"]\n",
        "    else:\n",
        "        assistant_message = response_message[\"content\"]\n",
        "\n",
        "    proc_messages.append({\"role\": \"assistant\", \"content\": assistant_message})\n",
        "    return proc_messages, assistant_message.strip()\n",
        "\n",
        "# Gradio 인터페이스\n",
        "messages = []\n",
        "\n",
        "def process(user_message, chat_history):\n",
        "    proc_messages, ai_message = ask_openai(\"gpt-4\", messages, user_message, functions=use_functions)\n",
        "    chat_history.append((user_message, ai_message))\n",
        "    return \"\", chat_history\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    chatbot = gr.Chatbot(label=\"센서 데이터 챗봇\")\n",
        "    user_textbox = gr.Textbox(label=\"입력\")\n",
        "    user_textbox.submit(process, [user_textbox, chatbot], [user_textbox, chatbot])\n",
        "\n",
        "demo.launch(share=True, debug=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "prQdkx8-nlL2",
        "outputId": "72ac39bd-024f-4700-e58c-8a40d528cdae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: openai==0.27.8 in /usr/local/lib/python3.10/dist-packages (0.27.8)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.27.8) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.27.8) (4.66.6)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.27.8) (3.11.9)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.27.8) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.27.8) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.27.8) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.27.8) (2024.8.30)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.27.8) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.27.8) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.27.8) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.27.8) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.27.8) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.27.8) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.27.8) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.27.8) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp->openai==0.27.8) (4.12.2)\n"
          ]
        }
      ],
      "source": [
        "pip install openai==0.27.8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2MQMcC4in7nU",
        "outputId": "aa16404e-67e1-450d-c86d-cffb4a07ad23"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gradio/components/chatbot.py:237: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://97dfc64af345222ac4.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://97dfc64af345222ac4.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/queueing.py\", line 624, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/route_utils.py\", line 323, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 2043, in process_api\n",
            "    result = await self.call_function(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1590, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/to_thread.py\", line 33, in run_sync\n",
            "    return await get_asynclib().run_sync_in_worker_thread(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 807, in run\n",
            "    result = context.run(func, *args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/utils.py\", line 865, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "  File \"<ipython-input-3-cacf687f8150>\", line 107, in process\n",
            "    proc_messages, ai_message = ask_openai(\"gpt-4-0613\", messages, user_message, functions=use_functions)\n",
            "  File \"<ipython-input-3-cacf687f8150>\", line 76, in ask_openai\n",
            "    response = openai.ChatCompletion.create(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
            "    return super().create(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
            "    response, _, api_key = requestor.request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\", line 298, in request\n",
            "    resp, got_stream = self._interpret_response(result, stream)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
            "    self._interpret_response_line(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\", line 763, in _interpret_response_line\n",
            "    raise self.handle_error_response(\n",
            "openai.error.InvalidRequestError: The model `gpt-4-0613` does not exist or you do not have access to it.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/queueing.py\", line 624, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/route_utils.py\", line 323, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 2043, in process_api\n",
            "    result = await self.call_function(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1590, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/to_thread.py\", line 33, in run_sync\n",
            "    return await get_asynclib().run_sync_in_worker_thread(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 807, in run\n",
            "    result = context.run(func, *args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/utils.py\", line 865, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "  File \"<ipython-input-3-cacf687f8150>\", line 107, in process\n",
            "    proc_messages, ai_message = ask_openai(\"gpt-4-0613\", messages, user_message, functions=use_functions)\n",
            "  File \"<ipython-input-3-cacf687f8150>\", line 76, in ask_openai\n",
            "    response = openai.ChatCompletion.create(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
            "    return super().create(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
            "    response, _, api_key = requestor.request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\", line 298, in request\n",
            "    resp, got_stream = self._interpret_response(result, stream)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
            "    self._interpret_response_line(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\", line 763, in _interpret_response_line\n",
            "    raise self.handle_error_response(\n",
            "openai.error.InvalidRequestError: The model `gpt-4-0613` does not exist or you do not have access to it.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://97dfc64af345222ac4.gradio.live\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#예전 open ai 버전을 설치후 실행\n",
        "\n",
        "# openai 버전이 맞지 않아 최신버전으로 수정한 코드, 이건 open ai 인터페이스가 1.0.0 이상 버전에서 변경됨, 코드가 구버전이라 최신 방식코드를\n",
        "#작성하거나 아니면 open ai 라이브러리의 이전 버전으로 설치해야함\n",
        "import pandas as pd\n",
        "import time\n",
        "import json\n",
        "import gradio as gr\n",
        "import openai\n",
        "\n",
        "# OpenAI API 키 설정\n",
        "openai.api_key = \"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\"\n",
        "\n",
        "# 센서 데이터 처리 함수 정의\n",
        "def moisture_sensor_info(mode='real-time', file_path=None, port='/dev/ttyUSB0', baudrate=9600):\n",
        "    if mode == 'real-time':\n",
        "        data = \"45\"\n",
        "        timestamp = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "        return str({'timestamp': timestamp, 'moisture': f\"{data}%\"})\n",
        "    elif mode == 'file':\n",
        "        df = pd.read_excel(file_path)\n",
        "        latest_entry = df.iloc[-1].to_dict()\n",
        "        return str({'timestamp': latest_entry['Timestamp'], 'moisture': f\"{latest_entry['Moisture (%)']}%\"})\n",
        "    else:\n",
        "        return str({'error': 'Invalid mode'})\n",
        "\n",
        "def light_sensor_info(mode='real-time', file_path=None, port='/dev/ttyUSB0', baudrate=9600):\n",
        "    if mode == 'real-time':\n",
        "        data = \"85\"\n",
        "        timestamp = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "        return str({'timestamp': timestamp, 'brightness': f\"{data}%\"})\n",
        "    elif mode == 'file':\n",
        "        df = pd.read_excel(file_path)\n",
        "        latest_entry = df.iloc[-1].to_dict()\n",
        "        return str({'timestamp': latest_entry['Timestamp'], 'brightness': f\"{latest_entry['Brightness (%)']}%\"})\n",
        "    else:\n",
        "        return str({'error': 'Invalid mode'})\n",
        "\n",
        "def dht11_sensor_info(mode='real-time', file_path=None, port='/dev/ttyUSB0', baudrate=9600):\n",
        "    if mode == 'real-time':\n",
        "        temperature = \"22\"\n",
        "        humidity = \"55\"\n",
        "        timestamp = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "        return str({'timestamp': timestamp, 'temperature': f\"{temperature}°C\", 'humidity': f\"{humidity}%\"})\n",
        "    elif mode == 'file':\n",
        "        df = pd.read_excel(file_path)\n",
        "        latest_entry = df.iloc[-1].to_dict()\n",
        "        return str({'timestamp': latest_entry['Timestamp'], 'temperature': f\"{latest_entry['Temperature (°C)']}°C\", 'humidity': f\"{latest_entry['Humidity (%)']}%\"})\n",
        "    else:\n",
        "        return str({'error': 'Invalid mode'})\n",
        "\n",
        "use_functions = [\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"moisture_sensor_info\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\"mode\": {\"type\": \"string\", \"enum\": [\"real-time\", \"file\"]}}\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "]\n",
        "\n",
        "def ask_openai(llm_model, messages, user_message, functions=''):\n",
        "    proc_messages = messages\n",
        "\n",
        "    if user_message:\n",
        "        proc_messages.append({\"role\": \"user\", \"content\": user_message})\n",
        "\n",
        "    if not functions:\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=llm_model, messages=proc_messages, temperature=1.0\n",
        "        )\n",
        "    else:\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=llm_model, messages=proc_messages, functions=functions, function_call=\"auto\"\n",
        "        )\n",
        "\n",
        "    response_message = response.choices[0].message\n",
        "    tool_calls = response_message.get(\"function_call\")\n",
        "\n",
        "    if tool_calls:\n",
        "        available_functions = {\n",
        "            \"moisture_sensor_info\": moisture_sensor_info,\n",
        "            \"light_sensor_info\": light_sensor_info,\n",
        "            \"dht11_sensor_info\": dht11_sensor_info,\n",
        "        }\n",
        "        function_name = tool_calls[\"name\"]\n",
        "        function_to_call = available_functions[function_name]\n",
        "        function_args = json.loads(tool_calls[\"arguments\"])\n",
        "        function_response = function_to_call(**function_args) if function_args else function_to_call()\n",
        "\n",
        "        proc_messages.append({\"role\": \"function\", \"name\": function_name, \"content\": function_response})\n",
        "        second_response = openai.ChatCompletion.create(model=llm_model, messages=proc_messages)\n",
        "        assistant_message = second_response.choices[0].message[\"content\"]\n",
        "    else:\n",
        "        assistant_message = response_message[\"content\"]\n",
        "\n",
        "    proc_messages.append({\"role\": \"assistant\", \"content\": assistant_message})\n",
        "    return proc_messages, assistant_message.strip()\n",
        "\n",
        "# Gradio 인터페이스\n",
        "messages = []\n",
        "\n",
        "def process(user_message, chat_history):\n",
        "    proc_messages, ai_message = ask_openai(\"gpt-4-0613\", messages, user_message, functions=use_functions)\n",
        "    chat_history.append((user_message, ai_message))\n",
        "    return \"\", chat_history\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    chatbot = gr.Chatbot(label=\"센서 데이터 챗봇\")\n",
        "    user_textbox = gr.Textbox(label=\"입력\")\n",
        "    user_textbox.submit(process, [user_textbox, chatbot], [user_textbox, chatbot])\n",
        "\n",
        "demo.launch(share=True, debug=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KsJtv4jJrlRl",
        "outputId": "80085a8d-a90e-43c6-bc96-a0207ffabc16"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gradio/components/chatbot.py:237: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://7198eb47113dd972e9.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://7198eb47113dd972e9.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/queueing.py\", line 624, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/route_utils.py\", line 323, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 2043, in process_api\n",
            "    result = await self.call_function(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1590, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/to_thread.py\", line 33, in run_sync\n",
            "    return await get_asynclib().run_sync_in_worker_thread(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 807, in run\n",
            "    result = context.run(func, *args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/utils.py\", line 865, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "  File \"<ipython-input-6-3539a1f78a99>\", line 85, in process\n",
            "    proc_messages, ai_message = ask_openai(\"gpt-3.5-turbo\", messages, user_message, functions=use_functions)\n",
            "  File \"<ipython-input-6-3539a1f78a99>\", line 54, in ask_openai\n",
            "    response = openai.ChatCompletion.create(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
            "    return super().create(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
            "    response, _, api_key = requestor.request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\", line 298, in request\n",
            "    resp, got_stream = self._interpret_response(result, stream)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
            "    self._interpret_response_line(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\", line 763, in _interpret_response_line\n",
            "    raise self.handle_error_response(\n",
            "openai.error.RateLimitError: You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://7198eb47113dd972e9.gradio.live\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 모델 이름 변경\n",
        "import pandas as pd\n",
        "import time\n",
        "import json\n",
        "import gradio as gr\n",
        "import openai\n",
        "\n",
        "# OpenAI API 키 설정\n",
        "openai.api_key = \"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\"  # 올바른 API 키를 입력하세요.\n",
        "\n",
        "# 센서 데이터 처리 함수 정의\n",
        "def moisture_sensor_info(mode='real-time', file_path=None, port='/dev/ttyUSB0', baudrate=9600):\n",
        "    if mode == 'real-time':\n",
        "        data = \"45\"\n",
        "        timestamp = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "        return str({'timestamp': timestamp, 'moisture': f\"{data}%\"})\n",
        "    elif mode == 'file':\n",
        "        df = pd.read_excel(file_path)\n",
        "        latest_entry = df.iloc[-1].to_dict()\n",
        "        return str({'timestamp': latest_entry['Timestamp'], 'moisture': f\"{latest_entry['Moisture (%)']}%\"})\n",
        "    else:\n",
        "        return str({'error': 'Invalid mode'})\n",
        "\n",
        "use_functions = [\n",
        "    {\n",
        "        \"name\": \"moisture_sensor_info\",\n",
        "        \"description\": \"Retrieve soil moisture data either in real-time or from a file.\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"mode\": {\"type\": \"string\", \"enum\": [\"real-time\", \"file\"]},\n",
        "                \"file_path\": {\"type\": \"string\", \"nullable\": True},\n",
        "                \"port\": {\"type\": \"string\", \"nullable\": True},\n",
        "                \"baudrate\": {\"type\": \"integer\", \"nullable\": True}\n",
        "            },\n",
        "            \"required\": [\"mode\"]\n",
        "        }\n",
        "    }\n",
        "]\n",
        "\n",
        "def ask_openai(llm_model, messages, user_message, functions=None):\n",
        "    proc_messages = messages\n",
        "\n",
        "    if user_message:\n",
        "        proc_messages.append({\"role\": \"user\", \"content\": user_message})\n",
        "\n",
        "    if not functions:\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=llm_model,\n",
        "            messages=proc_messages,\n",
        "            temperature=1.0\n",
        "        )\n",
        "    else:\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=llm_model,\n",
        "            messages=proc_messages,\n",
        "            functions=functions,\n",
        "            function_call=\"auto\"\n",
        "        )\n",
        "\n",
        "    response_message = response[\"choices\"][0][\"message\"]\n",
        "    function_call = response_message.get(\"function_call\")\n",
        "\n",
        "    if function_call:\n",
        "        available_functions = {\n",
        "            \"moisture_sensor_info\": moisture_sensor_info,\n",
        "        }\n",
        "        function_name = function_call[\"name\"]\n",
        "        function_to_call = available_functions[function_name]\n",
        "        function_args = json.loads(function_call[\"arguments\"])\n",
        "        function_response = function_to_call(**function_args)\n",
        "        proc_messages.append({\"role\": \"function\", \"name\": function_name, \"content\": function_response})\n",
        "        second_response = openai.ChatCompletion.create(model=llm_model, messages=proc_messages)\n",
        "        assistant_message = second_response[\"choices\"][0][\"message\"][\"content\"]\n",
        "    else:\n",
        "        assistant_message = response_message[\"content\"]\n",
        "\n",
        "    proc_messages.append({\"role\": \"assistant\", \"content\": assistant_message})\n",
        "    return proc_messages, assistant_message.strip()\n",
        "\n",
        "# Gradio 인터페이스\n",
        "messages = []\n",
        "\n",
        "def process(user_message, chat_history):\n",
        "    proc_messages, ai_message = ask_openai(\"gpt-3.5-turbo\", messages, user_message, functions=use_functions)\n",
        "    chat_history.append((user_message, ai_message))\n",
        "    return \"\", chat_history\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    chatbot = gr.Chatbot(label=\"센서 데이터 챗봇\")\n",
        "    user_textbox = gr.Textbox(label=\"입력\")\n",
        "    user_textbox.submit(process, [user_textbox, chatbot], [user_textbox, chatbot])\n",
        "\n",
        "demo.launch(share=True, debug=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        },
        "id": "7FM8gID9tkR7",
        "outputId": "b64aabbd-f5a9-41f6-9911-596dc088647e"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'gradio'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-554ccee10d99>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgradio\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mopenai\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gradio'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "#다시 수정\n",
        "\n",
        "import pandas as pd\n",
        "import time\n",
        "import json\n",
        "import gradio as gr\n",
        "import openai\n",
        "\n",
        "# OpenAI API 키 설정\n",
        "openai.api_key = \"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\"  # 올바른 API 키 입력\n",
        "\n",
        "# 센서 데이터 처리 함수 정의\n",
        "def moisture_sensor_info(mode='real-time', file_path=None, port='/dev/ttyUSB0', baudrate=9600):\n",
        "    if mode == 'real-time':\n",
        "        data = \"45\"\n",
        "        timestamp = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "        return str({'timestamp': timestamp, 'moisture': f\"{data}%\"})\n",
        "    elif mode == 'file':\n",
        "        df = pd.read_excel(file_path)\n",
        "        latest_entry = df.iloc[-1].to_dict()\n",
        "        return str({'timestamp': latest_entry['Timestamp'], 'moisture': f\"{latest_entry['Moisture (%)']}%\"})\n",
        "    else:\n",
        "        return str({'error': 'Invalid mode'})\n",
        "\n",
        "use_functions = [\n",
        "    {\n",
        "        \"name\": \"moisture_sensor_info\",\n",
        "        \"description\": \"Retrieve soil moisture data either in real-time or from a file.\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"mode\": {\"type\": \"string\", \"enum\": [\"real-time\", \"file\"]},\n",
        "                \"file_path\": {\"type\": \"string\", \"nullable\": True},\n",
        "                \"port\": {\"type\": \"string\", \"nullable\": True},\n",
        "                \"baudrate\": {\"type\": \"integer\", \"nullable\": True}\n",
        "            },\n",
        "            \"required\": [\"mode\"]\n",
        "        }\n",
        "    }\n",
        "]\n",
        "\n",
        "def ask_openai(llm_model, messages, user_message, functions=None):\n",
        "    proc_messages = messages\n",
        "\n",
        "    if user_message:\n",
        "        proc_messages.append({\"role\": \"user\", \"content\": user_message})\n",
        "\n",
        "    try:\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=llm_model,\n",
        "            messages=proc_messages,\n",
        "            functions=functions,\n",
        "            function_call=\"auto\"\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(\"OpenAI API Error:\", str(e))\n",
        "        return proc_messages, \"An error occurred while processing your request.\"\n",
        "\n",
        "    response_message = response[\"choices\"][0][\"message\"]\n",
        "    function_call = response_message.get(\"function_call\")\n",
        "\n",
        "    if function_call:\n",
        "        available_functions = {\n",
        "            \"moisture_sensor_info\": moisture_sensor_info,\n",
        "        }\n",
        "        function_name = function_call[\"name\"]\n",
        "        function_to_call = available_functions[function_name]\n",
        "        function_args = json.loads(function_call[\"arguments\"])\n",
        "        function_response = function_to_call(**function_args)\n",
        "        proc_messages.append({\"role\": \"function\", \"name\": function_name, \"content\": function_response})\n",
        "        second_response = openai.ChatCompletion.create(model=llm_model, messages=proc_messages)\n",
        "        assistant_message = second_response[\"choices\"][0][\"message\"][\"content\"]\n",
        "    else:\n",
        "        assistant_message = response_message[\"content\"]\n",
        "\n",
        "    proc_messages.append({\"role\": \"assistant\", \"content\": assistant_message})\n",
        "    return proc_messages, assistant_message.strip()\n",
        "\n",
        "# Gradio 인터페이스\n",
        "messages = []\n",
        "\n",
        "def process(user_message, chat_history):\n",
        "    proc_messages, ai_message = ask_openai(\"gpt-3.5-turbo\", messages, user_message, functions=use_functions)\n",
        "    chat_history.append((user_message, ai_message))\n",
        "    return \"\", chat_history\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    chatbot = gr.Chatbot(label=\"센서 데이터 챗봇\")\n",
        "    user_textbox = gr.Textbox(label=\"입력\")\n",
        "    user_textbox.submit(process, [user_textbox, chatbot], [user_textbox, chatbot])\n",
        "\n",
        "demo.launch(share=True, debug=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DcTR35S11-uF"
      },
      "source": [
        "# 12월 11일\n",
        "작성한 센서 관련 코드를 사용하여 gradio ui 생성\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eVfxlk962GsZ",
        "outputId": "5effd7ce-f8eb-4336-9652-ceb9483f0c6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.54.5)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.28.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.8.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.10.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.27.1)\n",
            "Collecting gradio\n",
            "  Downloading gradio-5.8.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.6-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.4.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting gradio-client==1.5.1 (from gradio)\n",
            "  Downloading gradio_client-1.5.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.28.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.25.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.26.3)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n",
            "Collecting markupsafe~=2.0 (from gradio)\n",
            "  Downloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.26.4)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.10.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (11.0.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.10.3)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio)\n",
            "  Downloading python_multipart-0.0.19-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.2.2 (from gradio)\n",
            "  Downloading ruff-0.8.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.41.3-py3-none-any.whl.metadata (6.0 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.15.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.2)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.32.1-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.5.1->gradio) (2024.10.0)\n",
            "Collecting websockets<15.0,>=10.0 (from gradio-client==1.5.1->gradio)\n",
            "  Downloading websockets-14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (3.16.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (4.66.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.27.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.1->gradio) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.1->gradio) (2.2.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-5.8.0-py3-none-any.whl (57.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.2/57.2 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.5.1-py3-none-any.whl (320 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.2/320.2 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.6-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "Downloading python_multipart-0.0.19-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.8.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m68.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.41.3-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading uvicorn-0.32.1-py3-none-any.whl (63 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.8/63.8 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.4.0-py3-none-any.whl (5.8 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Downloading websockets-14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.2/168.2 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pydub, websockets, uvicorn, tomlkit, semantic-version, ruff, python-multipart, markupsafe, ffmpy, aiofiles, starlette, safehttpx, gradio-client, fastapi, gradio\n",
            "  Attempting uninstall: markupsafe\n",
            "    Found existing installation: MarkupSafe 3.0.2\n",
            "    Uninstalling MarkupSafe-3.0.2:\n",
            "      Successfully uninstalled MarkupSafe-3.0.2\n",
            "Successfully installed aiofiles-23.2.1 fastapi-0.115.6 ffmpy-0.4.0 gradio-5.8.0 gradio-client-1.5.1 markupsafe-2.1.5 pydub-0.25.1 python-multipart-0.0.19 ruff-0.8.2 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.41.3 tomlkit-0.13.2 uvicorn-0.32.1 websockets-14.1\n",
            "Collecting pyserial\n",
            "  Downloading pyserial-3.5-py2.py3-none-any.whl.metadata (1.6 kB)\n",
            "Downloading pyserial-3.5-py2.py3-none-any.whl (90 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.6/90.6 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyserial\n",
            "Successfully installed pyserial-3.5\n"
          ]
        }
      ],
      "source": [
        "# 필수 라이브러리 설치\n",
        "!pip install openai\n",
        "!pip install gradio\n",
        "!pip install pyserial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1F4HKmoZ3q4R"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Import\n",
        "import gradio as gr\n",
        "import pandas as pd\n",
        "import serial\n",
        "import time\n",
        "import json\n",
        "from openai import OpenAI\n",
        "\n",
        "# OpenAI API 키 설정\n",
        "OpenAI.api_key = \"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\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Z2jFfOKs2Kpa"
      },
      "outputs": [],
      "source": [
        "def moisture_sensor_info(mode='real-time', file_path=None, port='/dev/ttyUSB0', baudrate=9600):\n",
        "    \"\"\"\n",
        "    토양 수분 센서 데이터를 처리하고 반환하는 함수.\n",
        "    \"\"\"\n",
        "    if mode == 'real-time':\n",
        "        try:\n",
        "            # 직렬 통신 설정\n",
        "            arduino = serial.Serial(port, baudrate, timeout=1)\n",
        "            if arduino.in_waiting > 0:\n",
        "                data = arduino.readline().decode('utf-8').strip()  # 데이터 읽기\n",
        "                timestamp = time.strftime(\"%Y-%m-%d %H:%M:%S\")   # 현재 시간\n",
        "                arduino.close()\n",
        "                return str({'timestamp': timestamp, 'moisture': f\"{data}%\"})\n",
        "            else:\n",
        "                return str({'error': 'No data available from sensor.'})\n",
        "        except Exception as e:\n",
        "            return str({'error': str(e)})\n",
        "\n",
        "    elif mode == 'file':\n",
        "        try:\n",
        "            if not file_path:\n",
        "                return str({'error': 'File path is required in file mode.'})\n",
        "            df = pd.read_excel(file_path)\n",
        "            if df.empty:\n",
        "                return str({'error': 'No data available in the file.'})\n",
        "            latest_entry = df.iloc[-1].to_dict()\n",
        "            return str({'timestamp': latest_entry['Timestamp'], 'moisture': f\"{latest_entry['Moisture (%)']}%\"})\n",
        "        except Exception as e:\n",
        "            return str({'error': str(e)})\n",
        "\n",
        "    else:\n",
        "        return str({'error': 'Invalid mode. Use \"real-time\" or \"file\".'})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "tyUimMso2Slw"
      },
      "outputs": [],
      "source": [
        "def light_sensor_info(mode='real-time', file_path=None, port='/dev/ttyUSB0', baudrate=9600):\n",
        "    \"\"\"\n",
        "    조도 센서 데이터를 처리하고 반환하는 함수.\n",
        "    \"\"\"\n",
        "    if mode == 'real-time':\n",
        "        try:\n",
        "            # 직렬 통신 설정\n",
        "            arduino = serial.Serial(port, baudrate, timeout=1)\n",
        "            time.sleep(2)  # 포트 안정화\n",
        "            if arduino.in_waiting > 0:\n",
        "                data = arduino.readline().decode(\"utf-8\").strip()  # 데이터 읽기\n",
        "                timestamp = time.strftime(\"%Y-%m-%d %H:%M:%S\")    # 현재 시간\n",
        "                arduino.close()\n",
        "                return str({'timestamp': timestamp, 'brightness': f\"{data.split(': ')[1]}%\"})\n",
        "            else:\n",
        "                return str({'error': 'No data available from sensor.'})\n",
        "        except Exception as e:\n",
        "            return str({'error': str(e)})\n",
        "\n",
        "    elif mode == 'file':\n",
        "        try:\n",
        "            if not file_path:\n",
        "                return str({'error': 'File path is required in file mode.'})\n",
        "            df = pd.read_excel(file_path)\n",
        "            if df.empty:\n",
        "                return str({'error': 'No data available in the file.'})\n",
        "            latest_entry = df.iloc[-1].to_dict()\n",
        "            return str({'timestamp': latest_entry['Timestamp'], 'brightness': f\"{latest_entry['Brightness (%)']}%\"})\n",
        "        except Exception as e:\n",
        "            return str({'error': str(e)})\n",
        "\n",
        "    else:\n",
        "        return str({'error': 'Invalid mode. Use \"real-time\" or \"file\".'})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "z4hkaeVQ2TcT"
      },
      "outputs": [],
      "source": [
        "sensor_functions = [\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"moisture_sensor_info\",\n",
        "            \"description\": \"Retrieves soil moisture data either in real-time from a sensor or from a file.\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"mode\": {\"type\": \"string\", \"description\": \"'real-time' for live data from the sensor or 'file' for data from an Excel file.\", \"enum\": [\"real-time\", \"file\"]},\n",
        "                    \"file_path\": {\"type\": \"string\", \"description\": \"The path to the Excel file containing soil moisture data (required if mode is 'file').\", \"nullable\": True},\n",
        "                    \"port\": {\"type\": \"string\", \"description\": \"The serial port to which the sensor is connected (required if mode is 'real-time'). Default is '/dev/ttyUSB0'.\", \"nullable\": True},\n",
        "                    \"baudrate\": {\"type\": \"integer\", \"description\": \"The baud rate for serial communication with the sensor. Default is 9600.\", \"nullable\": True}\n",
        "                },\n",
        "                \"required\": [\"mode\"]\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"light_sensor_info\",\n",
        "            \"description\": \"Retrieves light sensor data either in real-time from a sensor or from a file.\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"mode\": {\"type\": \"string\", \"description\": \"'real-time' for live data from the sensor or 'file' for data from an Excel file.\", \"enum\": [\"real-time\", \"file\"]},\n",
        "                    \"file_path\": {\"type\": \"string\", \"description\": \"The path to the Excel file containing brightness data (required if mode is 'file').\", \"nullable\": True},\n",
        "                    \"port\": {\"type\": \"string\", \"description\": \"The serial port to which the sensor is connected (required if mode is 'real-time'). Default is '/dev/ttyUSB0'.\", \"nullable\": True},\n",
        "                    \"baudrate\": {\"type\": \"integer\", \"description\": \"The baud rate for serial communication with the sensor. Default is 9600.\", \"nullable\": True}\n",
        "                },\n",
        "                \"required\": [\"mode\"]\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "RG5vBv902U9R"
      },
      "outputs": [],
      "source": [
        "def ask_openai(llm_model, messages, user_message, functions):\n",
        "    client = OpenAI()\n",
        "    proc_messages = messages\n",
        "\n",
        "    if user_message:\n",
        "        proc_messages.append({\"role\": \"user\", \"content\": user_message})\n",
        "\n",
        "    response = client.chat.completions.create(model=llm_model, messages=proc_messages, tools=functions, tool_choice=\"auto\")\n",
        "    response_message = response.choices[0].message\n",
        "    tool_calls = response_message.tool_calls\n",
        "\n",
        "    if tool_calls:\n",
        "        available_functions = {\n",
        "            \"moisture_sensor_info\": moisture_sensor_info,\n",
        "            \"light_sensor_info\": light_sensor_info\n",
        "        }\n",
        "\n",
        "        proc_messages.append(response_message)\n",
        "\n",
        "        for tool_call in tool_calls:\n",
        "            function_name = tool_call.function.name\n",
        "            function_to_call = available_functions[function_name]\n",
        "            function_args = json.loads(tool_call.function.arguments)\n",
        "            function_response = function_to_call(**function_args)\n",
        "\n",
        "            proc_messages.append({\n",
        "                \"tool_call_id\": tool_call.id,\n",
        "                \"role\": \"tool\",\n",
        "                \"name\": function_name,\n",
        "                \"content\": function_response,\n",
        "            })\n",
        "\n",
        "        second_response = client.chat.completions.create(model=llm_model, messages=proc_messages)\n",
        "        assistant_message = second_response.choices[0].message.content\n",
        "    else:\n",
        "        assistant_message = response_message.content\n",
        "\n",
        "    proc_messages.append({\"role\": \"assistant\", \"content\": assistant_message})\n",
        "    return proc_messages, assistant_message\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ieLyqt3v2Xkt",
        "outputId": "2923a2a6-bbc3-4399-9aa2-3f4e730a6079"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gradio/components/chatbot.py:237: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://152debda83677a4cf5.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://152debda83677a4cf5.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/queueing.py\", line 624, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/route_utils.py\", line 323, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 2043, in process_api\n",
            "    result = await self.call_function(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1590, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/to_thread.py\", line 33, in run_sync\n",
            "    return await get_asynclib().run_sync_in_worker_thread(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 807, in run\n",
            "    result = context.run(func, *args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/utils.py\", line 865, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "  File \"<ipython-input-7-499fe5f9b1fb>\", line 4, in process\n",
            "    proc_messages, ai_message = ask_openai(\"gpt-4o-mini\", messages, user_message, functions=sensor_functions)\n",
            "  File \"<ipython-input-6-90f6ecaf5668>\", line 2, in ask_openai\n",
            "    client = OpenAI()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_client.py\", line 105, in __init__\n",
            "    raise OpenAIError(\n",
            "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://152debda83677a4cf5.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "messages = []\n",
        "\n",
        "def process(user_message, chat_history):\n",
        "    proc_messages, ai_message = ask_openai(\"gpt-4o-mini\", messages, user_message, functions=sensor_functions)\n",
        "    chat_history.append((user_message, ai_message))\n",
        "    return \"\", chat_history\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    chatbot = gr.Chatbot(label=\"센서 데이터 조회\")\n",
        "    user_textbox = gr.Textbox(label=\"입력\")\n",
        "    user_textbox.submit(process, [user_textbox, chatbot], [user_textbox, chatbot])\n",
        "\n",
        "demo.launch(share=True, debug=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 디스코드(웹훅 사용)\n",
        "위의 코드랑 통합한것"
      ],
      "metadata": {
        "id": "MwuO6H50NoLB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import pandas as pd\n",
        "import serial\n",
        "import time\n",
        "import json\n",
        "import requests\n",
        "from openai import OpenAI\n",
        "\n",
        "# OpenAI API 키 설정\n",
        "OpenAI.api_key = \"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\"\n",
        "\n",
        "# 디스코드 웹훅 URL 설정\n",
        "WEBHOOK_URL = \"https://discord.com/api/webhooks/your_webhook_id/your_webhook_token\"\n",
        "\n",
        "def send_to_discord(message, webhook_url):\n",
        "    \"\"\"\n",
        "    디스코드로 메시지를 전송하는 함수\n",
        "    :param message: 전송할 메시지\n",
        "    :param webhook_url: 디스코드 웹훅 URL\n",
        "    \"\"\"\n",
        "    try:\n",
        "        data = {\"content\": message}\n",
        "        response = requests.post(webhook_url, json=data)\n",
        "        if response.status_code == 204:\n",
        "            print(\"메시지가 성공적으로 전송되었습니다.\")\n",
        "        else:\n",
        "            print(f\"전송 실패! 상태 코드: {response.status_code}, 응답: {response.text}\")\n",
        "    except Exception as e:\n",
        "        print(f\"오류 발생: {e}\")\n",
        "\n",
        "def moisture_sensor_info(mode='real-time', file_path=None, port='/dev/ttyUSB0', baudrate=9600):\n",
        "    \"\"\"\n",
        "    토양 수분 센서 데이터를 처리하고 반환하는 함수.\n",
        "    \"\"\"\n",
        "    if mode == 'real-time':\n",
        "        try:\n",
        "            arduino = serial.Serial(port, baudrate, timeout=1)\n",
        "            if arduino.in_waiting > 0:\n",
        "                data = arduino.readline().decode('utf-8').strip()\n",
        "                timestamp = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "                arduino.close()\n",
        "                return {'timestamp': timestamp, 'moisture': f\"{data}%\"}\n",
        "            else:\n",
        "                return {'error': 'No data available from sensor.'}\n",
        "        except Exception as e:\n",
        "            return {'error': str(e)}\n",
        "\n",
        "    elif mode == 'file':\n",
        "        try:\n",
        "            if not file_path:\n",
        "                return {'error': 'File path is required in file mode.'}\n",
        "            df = pd.read_excel(file_path)\n",
        "            if df.empty:\n",
        "                return {'error': 'No data available in the file.'}\n",
        "            latest_entry = df.iloc[-1].to_dict()\n",
        "            return {'timestamp': latest_entry['Timestamp'], 'moisture': f\"{latest_entry['Moisture (%)']}%\"}\n",
        "        except Exception as e:\n",
        "            return {'error': str(e)}\n",
        "\n",
        "    else:\n",
        "        return {'error': 'Invalid mode. Use \"real-time\" or \"file\".'}\n",
        "\n",
        "def light_sensor_info(mode='real-time', file_path=None, port='/dev/ttyUSB0', baudrate=9600):\n",
        "    \"\"\"\n",
        "    조도 센서 데이터를 처리하고 반환하는 함수.\n",
        "    \"\"\"\n",
        "    if mode == 'real-time':\n",
        "        try:\n",
        "            arduino = serial.Serial(port, baudrate, timeout=1)\n",
        "            time.sleep(2)\n",
        "            if arduino.in_waiting > 0:\n",
        "                data = arduino.readline().decode(\"utf-8\").strip()\n",
        "                timestamp = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "                arduino.close()\n",
        "                return {'timestamp': timestamp, 'brightness': f\"{data.split(': ')[1]}%\"}\n",
        "            else:\n",
        "                return {'error': 'No data available from sensor.'}\n",
        "        except Exception as e:\n",
        "            return {'error': str(e)}\n",
        "\n",
        "    elif mode == 'file':\n",
        "        try:\n",
        "            if not file_path:\n",
        "                return {'error': 'File path is required in file mode.'}\n",
        "            df = pd.read_excel(file_path)\n",
        "            if df.empty:\n",
        "                return {'error': 'No data available in the file.'}\n",
        "            latest_entry = df.iloc[-1].to_dict()\n",
        "            return {'timestamp': latest_entry['Timestamp'], 'brightness': f\"{latest_entry['Brightness (%)']}%\"}\n",
        "        except Exception as e:\n",
        "            return {'error': str(e)}\n",
        "\n",
        "    else:\n",
        "        return {'error': 'Invalid mode. Use \"real-time\" or \"file\".'}\n",
        "\n",
        "def ask_openai(llm_model, messages, user_message, functions):\n",
        "    client = OpenAI()\n",
        "    proc_messages = messages\n",
        "\n",
        "    if user_message:\n",
        "        proc_messages.append({\"role\": \"user\", \"content\": user_message})\n",
        "\n",
        "    response = client.chat.completions.create(model=llm_model, messages=proc_messages, tools=functions, tool_choice=\"auto\")\n",
        "    response_message = response.choices[0].message\n",
        "    tool_calls = response_message.tool_calls\n",
        "\n",
        "    if tool_calls:\n",
        "        available_functions = {\n",
        "            \"moisture_sensor_info\": moisture_sensor_info,\n",
        "            \"light_sensor_info\": light_sensor_info\n",
        "        }\n",
        "\n",
        "        proc_messages.append(response_message)\n",
        "\n",
        "        for tool_call in tool_calls:\n",
        "            function_name = tool_call.function.name\n",
        "            function_to_call = available_functions[function_name]\n",
        "            function_args = json.loads(tool_call.function.arguments)\n",
        "            function_response = function_to_call(**function_args)\n",
        "\n",
        "            proc_messages.append({\n",
        "                \"tool_call_id\": tool_call.id,\n",
        "                \"role\": \"tool\",\n",
        "                \"name\": function_name,\n",
        "                \"content\": function_response,\n",
        "            })\n",
        "\n",
        "            # 디스코드 알림 전송\n",
        "            send_to_discord(f\"센서 데이터 알림: {function_response}\", WEBHOOK_URL)\n",
        "\n",
        "        second_response = client.chat.completions.create(model=llm_model, messages=proc_messages)\n",
        "        assistant_message = second_response.choices[0].message.content\n",
        "    else:\n",
        "        assistant_message = response_message.content\n",
        "\n",
        "    proc_messages.append({\"role\": \"assistant\", \"content\": assistant_message})\n",
        "    return proc_messages, assistant_message\n",
        "\n",
        "messages = []\n",
        "\n",
        "def process(user_message, chat_history):\n",
        "    proc_messages, ai_message = ask_openai(\"gpt-4o-mini\", messages, user_message, functions=sensor_functions)\n",
        "    chat_history.append((user_message, ai_message))\n",
        "    return \"\", chat_history\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    chatbot = gr.Chatbot(label=\"센서 데이터 조회\")\n",
        "    user_textbox = gr.Textbox(label=\"입력\")\n",
        "    user_textbox.submit(process, [user_textbox, chatbot], [user_textbox, chatbot])\n",
        "\n",
        "demo.launch(share=True, debug=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 651
        },
        "id": "dzRKZGB2L5Ce",
        "outputId": "8bf5f5c5-6b0d-425d-9941-d6fff4b7e677"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gradio/components/chatbot.py:237: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://de3ef61bc72514bb42.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://de3ef61bc72514bb42.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}